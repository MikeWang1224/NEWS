# -*- coding: utf-8 -*-
"""
å¤šå…¬å¸æ–°èæŠ“å–ç¨‹å¼ï¼ˆå°ç©é›» + é´»æµ· + è¯é›»ï¼‰
ç‰ˆæœ¬ï¼šv6-groqï¼ˆembedding ç‰ˆï¼‰
------------------------------------------------
âœ” Firestore æª”ååªç”¨æ—¥æœŸï¼ˆç„¡æ™‚é–“å°¾ç¢¼ï¼‰
âœ” å„²å­˜æ–°è title + content + ç•¶æ—¥æ¼²è·Œ + embedding
âœ” ç”¨ yfinance æŠ“æ¼²è·Œ
âœ” Yahoo / TechNews / CNBC æŠ“å–ç©©å®š
âœ” è¯é›»æ–°èæ–°å¢ã€Œä»Šå¤© / æ˜¨å¤©ã€æ—¥æœŸéæ¿¾
âœ” embedding æ”¹ç”¨ Groq
"""

import os
import time
import json
import requests
from datetime import datetime
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
import warnings
import firebase_admin
from firebase_admin import credentials, firestore
import yfinance as yf
from groq import Groq  # <- Groq SDK

warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

# ---------------------- è¨­å®š ---------------------- #
headers = {
    'User-Agent': (
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/91.0.4472.124 Safari/537.36'
    )
}

# Firestore
key_dict = json.loads(os.environ["NEWS"])
cred = credentials.Certificate(key_dict)
firebase_admin.initialize_app(cred)
db = firestore.client()

# ---------------------- Groq client ---------------------- #
api_key = os.environ.get("GROQ_API_KEY")
if not api_key:
    raise ValueError("âš ï¸ æ‰¾ä¸åˆ° GROQ_API_KEYï¼Œè«‹ç¢ºèª Secret è¨­å®šæ­£ç¢ºï¼")

client = Groq(api_key=api_key)

# ---------------------- è‚¡ç¥¨å°æ‡‰ ---------------------- #
ticker_map = {
    "å°ç©é›»": "2330.TW",
    "é´»æµ·": "2317.TW",
    "è¯é›»": "2303.TW"
}

# ---------------------- æŠ“è‚¡åƒ¹æ¼²è·Œ ---------------------- #
def fetch_stock_change_yf(stock_name):
    ticker = ticker_map.get(stock_name)
    if not ticker:
        return "ç„¡è³‡æ–™"
    try:
        df = yf.Ticker(ticker).history(period="2d")
        if len(df) < 2:
            return "ç„¡è³‡æ–™"
        last_close = df['Close'][-1]
        prev_close = df['Close'][-2]
        change = last_close - prev_close
        pct = change / prev_close * 100
        sign = "+" if change >= 0 else ""
        return f"{sign}{change:.2f} ({sign}{pct:.2f}%)"
    except Exception:
        return "ç„¡è³‡æ–™"

def add_price_change(news_list, stock_name):
    change = fetch_stock_change_yf(stock_name)
    for news in news_list:
        news["price_change"] = change
    return news_list

# ---------------------- Embeddingï¼ˆGroqï¼‰ ---------------------- #
def generate_embedding(text):
    try:
        resp = client.embeddings.create(
            model="text-embedding-3-large",  # Groq å®˜æ–¹æä¾› embedding æ¨¡å‹
            input=text
        )
        return resp.data[0].embedding
    except Exception as e:
        print(f"âš ï¸ Groq embedding ç”Ÿæˆå¤±æ•—: {e}")
        return []

# ---------------------- å…±ç”¨å·¥å…· ---------------------- #
def fetch_article_content(url, source):
    try:
        r = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')

        if source == 'yahoo':
            paragraphs = soup.select('article p') or soup.select('p')
        elif source == 'cnbc':
            selectors = ['.ArticleBody-articleBody p', '.InlineArticleBody p', 'article p', 'p']
            for sel in selectors:
                paragraphs = soup.select(sel)
                if len(paragraphs) > 2:
                    break
        else:
            paragraphs = soup.select('div.entry-content p, div.entry-content h2')

        content = '\n'.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 40])
        return content[:1500] + ('...' if len(content) > 1500 else '')

    except Exception:
        return "ç„¡æ³•å–å¾—æ–°èå…§å®¹"

# ---------------------- TechNews ---------------------- #
def fetch_technews(keyword="å°ç©é›»", limit=10):
    print(f"\nğŸ“¡ æŠ“å– TechNewsï¼ˆ{keyword}ï¼‰...")
    search_url = f'https://technews.tw/google-search/?googlekeyword={keyword}'
    links, news = [], []

    try:
        res = requests.get(search_url, headers=headers)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')

        for a in soup.find_all('a', href=True):
            href = a['href']
            if href.startswith('https://technews.tw/') and all(
                x not in href for x in ['/tag/', '/page/', '/author/', '/videos/', '/about/', '/tn-rss/']
            ):
                if href not in links:
                    links.append(href)

        links = links[:limit]

    except Exception as e:
        print(f"âš ï¸ TechNews æŠ“å–å¤±æ•—: {e}")
        return []

    for link in links:
        try:
            r = requests.get(link, headers=headers)
            soup = BeautifulSoup(r.text, 'html.parser')
            title = soup.find('h1', class_='entry-title').get_text(strip=True)
            content = fetch_article_content(link, 'technews')
            news.append({'title': title, 'content': content})
            time.sleep(1)
        except Exception:
            continue

    return news

# ---------------------- Yahoo æ–°è ---------------------- #
def fetch_yahoo_news(keyword="å°ç©é›»", limit=5):
    print(f"\nğŸ“¡ æŠ“å– Yahoo æ–°èï¼ˆ{keyword}ï¼‰...")
    base_url = "https://tw.news.yahoo.com"
    search_url = f"{base_url}/search?p={keyword}&sort=time"

    news_list, seen_titles = [], set()

    try:
        resp = requests.get(search_url, headers=headers)
        soup = BeautifulSoup(resp.text, "html.parser")

        articles = soup.select('li[data-testid="search-result"] a.js-content-viewer') \
                   or soup.select('h3 a')

        for a in articles:
            if len(news_list) >= limit:
                break

            title = a.get_text(strip=True)
            if not title or title in seen_titles:
                continue
            seen_titles.add(title)

            href = a.get("href")
            if href and not href.startswith("http"):
                href = base_url + href

            summary = fetch_article_content(href, 'yahoo')
            news_list.append({'title': title, 'content': summary})

    except Exception as e:
        print(f"âš ï¸ Yahoo æŠ“å–å¤±æ•—: {e}")

    return news_list

# ---------------------- Yahoo Finance è¯é›» ---------------------- #
def fetch_umc_yahoo_official(limit=8):
    print("\nğŸ“¡ æŠ“å– Yahoo Finance è¯é›»æ–°èï¼ˆå®˜æ–¹é ï¼‰...")

    base_url = "https://tw.stock.yahoo.com"
    search_url = f"{base_url}/quote/2303.TW/news"

    news_list, seen_titles = [], set()

    today = datetime.now().date()
    yesterday = today.fromordinal(today.toordinal() - 1)

    try:
        resp = requests.get(search_url, headers=headers)
        soup = BeautifulSoup(resp.text, "html.parser")

        articles = soup.select('li.js-stream-content')

        for item in articles:
            if len(news_list) >= limit:
                break

            a = item.select_one('a')
            if not a:
                continue

            title = a.get_text(strip=True)
            if not title or title in seen_titles:
                continue

            time_tag = item.select_one('time')
            if not time_tag:
                continue

            date_str = time_tag.get('datetime', '')[:10]

            try:
                pub_date = datetime.strptime(date_str, "%Y-%m-%d").date()
            except:
                continue

            if pub_date not in [today, yesterday]:
                continue

            seen_titles.add(title)

            href = a.get("href")
            if href and not href.startswith("http"):
                href = base_url + href

            summary = fetch_article_content(href, 'yahoo')

            news_list.append({
                'title': title,
                'content': summary
            })

    except Exception as e:
        print(f"âš ï¸ Yahoo Finance è¯é›»æŠ“å–å¤±æ•—: {e}")

    return news_list

# ---------------------- CNBC ---------------------- #
def fetch_cnbc_news(keyword_list=["TSMC"], limit=8):
    print(f"\nğŸ“¡ æŠ“å– CNBC æ–°èï¼ˆ{'/'.join(keyword_list)}ï¼‰...")

    search_urls = [
        "https://www.cnbc.com/search/?query=" + '+'.join(keyword_list),
        "https://www.cnbc.com/technology/",
        "https://www.cnbc.com/semiconductors/"
    ]

    news_list, seen_titles = [], set()
    today = datetime.now().date()
    yesterday = today.fromordinal(today.toordinal() - 1)

    def extract_date(article):
        time_tag = article.find("time")
        if not time_tag:
            return None
        dt = time_tag.get("datetime", "")[:10]
        try:
            return datetime.strptime(dt, "%Y-%m-%d").date()
        except:
            return None

    for url in search_urls:

        if len(news_list) >= limit:
            break

        try:
            resp = requests.get(url, headers=headers)
            soup = BeautifulSoup(resp.text, 'html.parser')

            articles = soup.select("article")

            for art in articles:

                if len(news_list) >= limit:
                    break

                a = art.find("a")
                if not a:
                    continue

                title = a.get_text(strip=True)
                if not title or title in seen_titles:
                    continue

                if not any(x.lower() in title.lower() for x in keyword_list):
                    continue

                pub_date = extract_date(art)
                if pub_date not in [today, yesterday]:
                    continue

                seen_titles.add(title)

                href = a.get("href")
                if not href or '/video/' in href:
                    continue
                if not href.startswith("http"):
                    href = "https://www.cnbc.com" + href

                content = fetch_article_content(href, 'cnbc')

                news_list.append({
                    'title': title,
                    'content': content
                })

        except:
            continue

    return news_list[:limit]

# ---------------------- Firestore å„²å­˜ ---------------------- #
def save_news_to_firestore(all_news, collection_name="NEWS"):
    collection_ref = db.collection(collection_name)
    doc_id = datetime.now().strftime("%Y%m%d")
    doc_ref = collection_ref.document(doc_id)

    news_dict = {}
    for i, news in enumerate(all_news, start=1):
        embedding = generate_embedding(news.get("content", ""))
        news_dict[f"news_{i}"] = {
            "title": news.get("title", "ç„¡æ¨™é¡Œ"),
            "price_change": news.get("price_change", "ç„¡è³‡æ–™"),
            "content": news.get("content", "ç„¡å…§å®¹"),
            "embedding": embedding
        }

    doc_ref.set(news_dict)
    print(f"âœ… å·²å¯«å…¥ Firestoreï¼š{collection_name}/{doc_id}ï¼ˆç­†æ•¸ï¼š{len(all_news)}ï¼‰")

# ---------------------- ä¸»ç¨‹å¼ ---------------------- #
if __name__ == "__main__":

    # å°ç©é›»
    technews = fetch_technews("å°ç©é›»", limit=10)
    yahoo_news = fetch_yahoo_news("å°ç©é›»", limit=10)
    cnbc_news = fetch_cnbc_news(["TSMC"], limit=10)

    all_tsmc = technews + yahoo_news + cnbc_news
    if all_tsmc:
        all_tsmc = add_price_change(all_tsmc, "å°ç©é›»")
        save_news_to_firestore(all_tsmc, "NEWS")

    # é´»æµ·
    honhai_news = fetch_yahoo_news("é´»æµ·", limit=15)
    if honhai_news:
        honhai_news = add_price_change(honhai_news, "é´»æµ·")
        save_news_to_firestore(honhai_news, "NEWS_Foxxcon")

    # è¯é›»
    umc_yahoo = fetch_umc_yahoo_official(limit=10)
    umc_tech = fetch_technews("è¯é›»", limit=8)
    umc_cnbc = fetch_cnbc_news(["UMC","United Microelectronics","è¯é›»"], limit=6)

    umc_news = umc_yahoo + umc_tech + umc_cnbc
    if umc_news:
        umc_news = add_price_change(umc_news, "è¯é›»")
        save_news_to_firestore(umc_news, "NEWS_UMC")

    print("\nğŸ‰ å…¨éƒ¨æ–°èæŠ“å–å®Œæˆï¼")
